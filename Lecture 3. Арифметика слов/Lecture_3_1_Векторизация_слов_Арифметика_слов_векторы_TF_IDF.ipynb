{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3KyBOrHScdmZCdfjDiaWM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/NLP-2024-2025/blob/main/Lecture_3_1_%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F_%D1%81%D0%BB%D0%BE%D0%B2_%D0%90%D1%80%D0%B8%D1%84%D0%BC%D0%B5%D1%82%D0%B8%D0%BA%D0%B0_%D1%81%D0%BB%D0%BE%D0%B2_%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D1%8B_TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lecture 3. Векторизация слов. Арифметика слов: векторы TF-IDF\n",
        "\n",
        "\n",
        "#### Введение\n",
        "\n",
        "В данной лекции мы поговорим о векторных представлениях слов, особенно о методе TF-IDF, который является одним из важных инструментов в области обработки текстов. Мы рассмотрим различные подходы к представлению слов и их значений, а также узнаем, как эти методы применяются для анализа текстов и поиска релевантных документов.\n",
        "\n",
        "#### 1. Подсчет слов и частотностей термов для анализа смысла\n",
        "\n",
        "Первым шагом в анализе текста является подсчет частоты встречаемости каждого слова. Это позволяет понять, какие слова наиболее часто встречаются в тексте и, следовательно, могут нести ключевой смысл.\n"
      ],
      "metadata": {
        "id": "N4PsmfQrFgWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подсчет частоты встречаемости слов и анализ терминов являются основными этапами при работе с текстовыми данными. Давайте разберемся подробнее с несколькими примерами.\n",
        "\n",
        "### 1. Подсчет частоты встречаемости слов\n",
        "\n",
        "#### Пример 1: Простой подсчет слов в тексте\n",
        "\n",
        "Предположим, у нас есть следующий текст:\n",
        "\n",
        "\"Машина едет по дороге. Машина останавливается у дома. Дом находится в лесу.\"\n",
        "\n",
        "Для начала работы с этим текстом мы должны выполнить следующие шаги:\n",
        "\n",
        "1. **Токенизация**: Разделить текст на отдельные слова или токены. В результате получим список слов:\n",
        "   ```\n",
        "   [\"Машина\", \"едет\", \"по\", \"дороге\", \"Машина\", \"останавливается\", \"у\", \"дома\", \"Дом\", \"находится\", \"в\", \"лесу\"]\n",
        "   ```\n",
        "\n",
        "2. **Подсчет частоты слов**: Посчитать, сколько раз каждое слово встречается в тексте:\n",
        "\n",
        "   - \"Машина\": 2 раза\n",
        "   - \"едет\": 1 раз\n",
        "   - \"по\": 1 раз\n",
        "   - \"дороге\": 1 раз\n",
        "   - \"останавливается\": 1 раз\n",
        "   - \"у\": 1 раз\n",
        "   - \"дома\": 1 раз\n",
        "   - \"Дом\": 1 раз\n",
        "   - \"находится\": 1 раз\n",
        "   - \"в\": 1 раз\n",
        "   - \"лесу\": 1 раз\n",
        "\n",
        "Эта информация позволяет нам понять, какие слова наиболее часто употребляются в тексте, и их вклад в общее содержание.\n",
        "\n",
        "#### Пример 2: Обработка текста из большой коллекции\n",
        "\n",
        "Предположим, у нас есть набор документов, и мы хотим выяснить, какие термины наиболее часто встречаются во всех документах.\n",
        "\n",
        "Для этого мы объединяем все тексты в один большой корпус, токенизируем его и подсчитываем частоту каждого слова. Например, если у нас есть два документа:\n",
        "\n",
        "**Документ 1:**\n",
        "```\n",
        "\"Искусственный интеллект меняет мир. Машины могут обучаться самостоятельно.\"\n",
        "```\n",
        "\n",
        "**Документ 2:**\n",
        "```\n",
        "\"Интеллектуальные системы имеют большое будущее. Интеллект может быть синонимом разума.\"\n",
        "```\n",
        "\n",
        "Объединенный корпус:\n",
        "```\n",
        "\"Искусственный интеллект меняет мир. Машины могут обучаться самостоятельно. Интеллектуальные системы имеют большое будущее. Интеллект может быть синонимом разума.\"\n",
        "```\n",
        "\n",
        "Токенизируем и подсчитаем частоту слов:\n",
        "\n",
        "   - \"интеллект\": 3 раза\n",
        "   - \"могут\": 2 раза\n",
        "   - \"быть\": 1 раз\n",
        "   - \"искусственный\": 1 раз\n",
        "   - \"меняет\": 1 раз\n",
        "   - \"мир\": 1 раз\n",
        "   - \"машины\": 1 раз\n",
        "   - \"обучаться\": 1 раз\n",
        "   - \"самостоятельно\": 1 раз\n",
        "   - \"интеллектуальные\": 1 раз\n",
        "   - \"системы\": 1 раз\n",
        "   - \"имеют\": 1 раз\n",
        "   - \"большое\": 1 раз\n",
        "   - \"будущее\": 1 раз\n",
        "   - \"может\": 1 раз\n",
        "   - \"синонимом\": 1 раз\n",
        "   - \"разума\": 1 раз\n",
        "\n",
        "Это позволяет нам увидеть, что \"интеллект\" встречается чаще всего, что может указывать на его ключевую роль в контексте данных текстов.\n",
        "\n",
        "Пример"
      ],
      "metadata": {
        "id": "ETjnVzVcGjVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def count_word_frequencies(text):\n",
        "    # Приводим текст к нижнему регистру для учета слов без учета регистра\n",
        "    text = text.lower()\n",
        "    # Используем регулярное выражение для извлечения слов (токенизация)\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    # Считаем частоту каждого слова с помощью Counter из модуля collections\n",
        "    word_counts = Counter(words)\n",
        "    return word_counts\n",
        "\n",
        "# Пример текста для анализа\n",
        "text = \"\"\"\n",
        "Машина едет по дороге. Машина останавливается у дома.\n",
        "Дом находится в лесу. Машина и дома разные понятия.\n",
        "\"\"\"\n",
        "\n",
        "# Получаем частоту встречаемости слов в тексте\n",
        "word_frequency = count_word_frequencies(text)\n",
        "\n",
        "# Выводим результаты\n",
        "print(\"Частота встречаемости слов в тексте:\")\n",
        "for word, count in word_frequency.items():\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oe20y5szGt4C",
        "outputId": "7952049a-89f3-4423-8449-2c9b27948652"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Частота встречаемости слов в тексте:\n",
            "машина: 3\n",
            "едет: 1\n",
            "по: 1\n",
            "дороге: 1\n",
            "останавливается: 1\n",
            "у: 1\n",
            "дома: 2\n",
            "дом: 1\n",
            "находится: 1\n",
            "в: 1\n",
            "лесу: 1\n",
            "и: 1\n",
            "разные: 1\n",
            "понятия: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Таким образом, подсчет частоты встречаемости слов — это важный этап анализа текста, который позволяет выявить ключевые термины и понять, какие идеи или концепции наиболее часто упоминаются в текстовых данных. Этот процесс является основой для более глубокого понимания содержания и структуры текста, а также для дальнейших аналитических и информационных задач."
      ],
      "metadata": {
        "id": "EPtlLScnGuM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 2. Предсказание вероятностей вхождений слов с помощью закона Ципфа\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hea98sWuGW0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Закон Ципфа, названный в честь американского лингвиста Джорджа Ципфа, описывает явление, согласно которому частота встречаемости слов в текстах часто следует степенному закону. Это означает, что частота f(r) встречаемости слова, занимающего r-ое место по частотности, обратно пропорциональна его рангу r:\n",
        "\n",
        "$$ f(r) \\propto \\frac{1}{r^\\alpha} $$\n",
        "\n",
        "где α - параметр, зависящий от конкретного текста.\n",
        "\n",
        "С другими словами закон Ципфа утверждает, что частота встречаемости слов в текстах обычно обратно пропорциональна их порядковому номеру по частотности. Это значит, что самое часто встречающееся слово встречается в тексте вдвое чаще, чем второе по частоте и т.д. Этот закон помогает предсказывать частоту вхождения слов и определять их важность в тексте.\n",
        "\n",
        "### Примеры и применение закона Ципфа:\n",
        "\n",
        "1. **В анализе текстов**:\n",
        "   - При анализе больших текстовых корпусов или коллекций документов можно применять закон Ципфа для оценки значимости слов. Например, если в тексте самое частое слово встречается 100 раз, то второе по частоте слово будет встречаться примерно 50 раз, третье - 33 раза, и так далее, что хорошо соответствует закону Ципфа.\n",
        "\n",
        "2. **В построении индексов и поисковых систем**:\n",
        "   - Предположим, что у нас есть поисковая система, которая должна определять важность слов в документах. Закон Ципфа позволяет оценивать, какие слова являются наиболее часто встречающимися и, следовательно, могут быть менее информативными (стоп-слова), а какие более важными (ключевые слова).\n",
        "\n",
        "3. **В статистическом анализе текстов**:\n",
        "   - При сравнении текстов на разных языках или в разных жанрах можно использовать закон Ципфа для изучения различий в частоте употребления слов. Например, можно выявить, что для одного жанра текстов (например, научных статей) закон Ципфа хорошо соблюдается, а для другого (например, художественных произведений) он может быть менее точным из-за специфики лексики.\n",
        "\n",
        "4. **В моделировании языковых процессов**:\n",
        "   - Моделирование процессов генерации текста (например, с использованием языковых моделей) может учитывать закон Ципфа для генерации реалистичных текстов по частоте встречаемости слов.\n",
        "\n"
      ],
      "metadata": {
        "id": "9Sr1YtjeIQjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте рассмотрим применение закона Ципфа к конкретному тексту с использованием формулы, которая описывает зависимость частоты встречаемости слов от их ранга.\n",
        "\n",
        "Предположим, у нас есть текст:\n",
        "\n",
        "**Текст:**\n",
        "\"Вчера вечером я пошел на концерт своей любимой группы. Музыка была потрясающей, и атмосфера была полностью волшебной. Я чувствовал, что каждая нота касается моей души. Это был незабываемый вечер.\"\n",
        "\n",
        "Теперь давайте посчитаем частоту встречаемости слов и проверим, соответствует ли она закону Ципфа.\n",
        "\n",
        "1. **Выделение слов из текста и подсчет их частоты:**\n",
        "\n",
        "   - \"вчера\" - 1 раз\n",
        "   - \"вечером\" - 1 раз\n",
        "   - \"я\" - 2 раза\n",
        "   - \"пошел\" - 1 раз\n",
        "   - \"на\" - 1 раз\n",
        "   - \"концерт\" - 1 раз\n",
        "   - \"своей\" - 1 раз\n",
        "   - \"любимой\" - 1 раз\n",
        "   - \"группы\" - 1 раз\n",
        "   - \"музыка\" - 1 раз\n",
        "   - \"была\" - 2 раза\n",
        "   - \"потрясающей\" - 1 раз\n",
        "   - \"и\" - 1 раз\n",
        "   - \"атмосфера\" - 1 раз\n",
        "   - \"полностью\" - 1 раз\n",
        "   - \"волшебной\" - 1 раз\n",
        "   - \"чувствовал\" - 1 раз\n",
        "   - \"что\" - 1 раз\n",
        "   - \"каждая\" - 1 раз\n",
        "   - \"нота\" - 1 раз\n",
        "   - \"касается\" - 1 раз\n",
        "   - \"моей\" - 1 раз\n",
        "   - \"души\" - 1 раз\n",
        "   - \"это\" - 1 раз\n",
        "   - \"незабываемый\" - 1 раз\n",
        "\n",
        "2. **Определение ранга слов и расчет частоты встречаемости:**\n",
        "\n",
        "   - Самое частое слово (\"я\") встречается 2 раза.\n",
        "   - Второе по частоте слово (\"была\") также встречается 2 раза.\n",
        "   - Третье по частоте слово (\"был\") встречается 1 раз.\n",
        "   - Четвертое по частоте слово (\"в\") встречается 1 раз.\n",
        "   - Пятое по частоте слово (\"вечером\") встречается 1 раз.\n",
        "\n",
        "3. **Применение закона Ципфа:**\n",
        "\n",
        "   Закон Ципфа утверждает, что частота f(r) встречаемости слова, занимающего r-ое место по частотности, обратно пропорциональна его рангу r:\n",
        "\n",
        "   $$ f(r) = \\frac{C}{r^\\alpha} $$\n",
        "\n",
        "   где C - нормализующая константа, а α - параметр, обычно около 1.\n",
        "\n",
        "   Давайте проверим, как соотносится ранг слова с его частотой встречаемости:\n",
        "\n",
        "   - Первое слово \"я\": $ f(1) = \\frac{C}{1^\\alpha} = C $\n",
        "   - Второе слово \"была\": $ f(2) = \\frac{C}{2^\\alpha} $\n",
        "   - Третье слово \"был\": $ f(3) = \\frac{C}{3^\\alpha} $\n",
        "   - И так далее...\n",
        "\n",
        "   Для нашего конкретного текста можно определить параметр α, используя значения частот и рангов слов. Обычно α близко к 1 для большинства текстов, что соответствует наблюдаемому убыванию частоты встречаемости по мере увеличения ранга слова.\n",
        "\n",
        "Этот подход позволяет математически подтвердить применимость закона Ципфа к конкретному тексту и использовать его для анализа частотности слов в больших корпусах текстов или при сравнении различных языковых структур."
      ],
      "metadata": {
        "id": "Egf_ws9qIhgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для решения задачи подсчета частоты встречаемости слов в тексте и проверки закона Ципфа на Python можно написать следующую программу."
      ],
      "metadata": {
        "id": "WyzBsyHrI7SD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Текст для анализа\n",
        "text = \"Вчера вечером я пошел на концерт своей любимой группы. Музыка была потрясающей, и атмосфера была полностью волшебной. Я чувствовал, что каждая нота касается моей души. Это был незабываемый вечер.\"\n",
        "\n",
        "# Приведем все слова к нижнему регистру и разобъем на слова\n",
        "words = text.lower().split()\n",
        "\n",
        "# Подсчитаем частоту встречаемости слов\n",
        "word_freq = Counter(words)\n",
        "\n",
        "# Получим список слов и их частот\n",
        "word_list = list(word_freq.keys())\n",
        "freq_list = list(word_freq.values())\n",
        "\n",
        "# Отсортируем список слов по убыванию частоты\n",
        "sorted_indices = sorted(range(len(freq_list)), key=lambda k: freq_list[k], reverse=True)\n",
        "word_list_sorted = [word_list[idx] for idx in sorted_indices]\n",
        "freq_list_sorted = [freq_list[idx] for idx in sorted_indices]\n",
        "\n",
        "# Выведем список слов и их частот\n",
        "print(\"Список слов и их частот в тексте:\")\n",
        "for word, freq in zip(word_list_sorted, freq_list_sorted):\n",
        "    print(f\"{word}: {freq}\")\n",
        "\n",
        "# Построим график для закона Ципфа\n",
        "ranks = list(range(1, len(word_list_sorted) + 1))\n",
        "frequencies = freq_list_sorted\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(ranks, frequencies, align='center', alpha=0.5)\n",
        "plt.xlabel('Ранг слова')\n",
        "plt.ylabel('Частота встречаемости')\n",
        "plt.title('Закон Ципфа')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vXs2RjapI_yD",
        "outputId": "7c891e2e-2502-477c-b87e-63276c5382b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Список слов и их частот в тексте:\n",
            "я: 2\n",
            "была: 2\n",
            "вчера: 1\n",
            "вечером: 1\n",
            "пошел: 1\n",
            "на: 1\n",
            "концерт: 1\n",
            "своей: 1\n",
            "любимой: 1\n",
            "группы.: 1\n",
            "музыка: 1\n",
            "потрясающей,: 1\n",
            "и: 1\n",
            "атмосфера: 1\n",
            "полностью: 1\n",
            "волшебной.: 1\n",
            "чувствовал,: 1\n",
            "что: 1\n",
            "каждая: 1\n",
            "нота: 1\n",
            "касается: 1\n",
            "моей: 1\n",
            "души.: 1\n",
            "это: 1\n",
            "был: 1\n",
            "незабываемый: 1\n",
            "вечер.: 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Таким образом, закон Ципфа полезен для анализа текстовых данных, предсказания частоты вхождения слов и оценки их важности в контексте. Он находит применение не только в лингвистике и текстовой аналитике, но и в информационном поиске, компьютерной лингвистике и машинном обучении."
      ],
      "metadata": {
        "id": "ZgorrZ0-GZ5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Векторные представления слов и способы их использования\n",
        "\n",
        "Теперь давайте перейдем к векторным представлениям слов, которые являются более мощным инструментом для анализа текстов.\n",
        "\n",
        "- **Мультимножества слов** — это векторы, где каждая компонента соответствует частоте встречаемости конкретного слова в тексте. Например, если у нас есть текст с тремя словами \"apple\", \"banana\" и \"orange\", и \"apple\" встречается 5 раз, \"banana\" — 3 раза и \"orange\" — 2 раза, то вектор будет иметь вид (5, 3, 2).\n",
        "\n",
        "- **Мультимножества n-грамм** — это расширение концепции мультимножеств слов на последовательности слов определенной длины (n). Например, для n=2 (биграммы) мы можем рассматривать пары слов, для n=3 (триграммы) — тройки слов и т.д. Это позволяет учитывать контекстуальные зависимости между словами.\n",
        "\n",
        "- **Векторы TF-IDF** (Term Frequency - Inverse Document Frequency) — это мера, которая отражает важность слова в документе или в корпусе документов, учитывая как частоту встречаемости слова в документе (TF), так и обратную частотность слова в корпусе (IDF). TF-IDF позволяет выявлять ключевые слова, которые характеризуют содержание документа относительно всего корпуса.\n",
        "\n"
      ],
      "metadata": {
        "id": "vrbobj04GaE8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "#### Мультимножества слов\n",
        "\n",
        "Мультимножество слов (Bag of Words, BoW) представляет собой простой метод создания векторных представлений на основе частоты встречаемости слов в документе.\n",
        "\n",
        "1. **Создание мультимножества слов:**\n",
        "   - Предложение или документ разбивается на отдельные слова.\n",
        "   - Для каждого слова подсчитывается его частота в документе.\n",
        "   - Каждое слово представляется вектором, где каждая компонента вектора соответствует частоте слова в документе.\n",
        "\n",
        "Пример:\n",
        "Пусть у нас есть предложение: \"Кошка сидит на окне и мурлыкает.\"\n",
        "Мультимножество слов для этого предложения:\n",
        "\n",
        "$\\{ \\text{\"Кошка\": 1, \"сидит\": 1, \"на\": 1, \"окне\": 1, \"и\": 1, \"мурлыкает\": 1 \\} $\n",
        "\n",
        "2. **Проблемы мультимножеств слов:**\n",
        "   - Не учитывает порядок слов.\n",
        "   - Игнорирует семантические отношения между словами.\n",
        "\n",
        "#### Мультимножества n-грамм\n",
        "\n",
        "Мультимножества n-грамм расширяют концепцию мультимножеств слов, учитывая последовательности из n подряд идущих слов.\n",
        "\n",
        "1. **Создание мультимножества n-грамм:**\n",
        "   - Документ разбивается на последовательности из n слов (n-граммы).\n",
        "   - Для каждой n-граммы подсчитывается её частота в документе.\n",
        "   - Каждая n-грамма представляется вектором по аналогии с мультимножествами слов.\n",
        "\n",
        "Пример:\n",
        "Предложение: \"Кошка сидит на окне и мурлыкает.\"\n",
        "Мультимножество 2-грамм:\n",
        "$$ \\{ \\text{\"Кошка сидит\": 1, \"сидит на\": 1, \"на окне\": 1, \"окне и\": 1, \"и мурлыкает\": 1 \\} $$\n",
        "\n",
        "2. **Применение мультимножеств n-грамм:**\n",
        "   - Учет контекста и частоты встречаемости более длинных фраз.\n",
        "   - Более сложная модель, требующая больше вычислительных ресурсов.\n",
        "\n",
        "#### Векторы TF-IDF (Term Frequency - Inverse Document Frequency)\n",
        "\n",
        "Векторы TF-IDF используются для оценки важности слов в документе относительно коллекции документов.\n",
        "\n",
        "1. **Вычисление TF (Term Frequency):**\n",
        "   - TF отражает, как часто слово встречается в документе.\n",
        "  $$ \\text{TF}(t, d) = \\frac{\\text{число раз, когда слово } t \\text{ встречается в документе } d}{\\text{общее число слов в документе } d} $$\n",
        "\n",
        "2. **Вычисление IDF (Inverse Document Frequency):**\n",
        "   - IDF измеряет, насколько информативным является слово по всей коллекции документов.\n",
        "   $$ \\text{IDF}(t, D) = \\log \\left( \\frac{|D|}{|\\{d \\in D : t \\in d\\}|} \\right) $$\n",
        "   где $ |D| $ - общее количество документов, $ |\\{d \\in D : t \\in d\\}| $ - количество документов, содержащих слово $ t $.\n",
        "\n",
        "3. **Вычисление вектора TF-IDF:**\n",
        "   - TF-IDF вектор для слова $t $ в документе $$ d $$:\n",
        "  $$ \\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\cdot \\text{IDF}(t, D) $$\n",
        "\n",
        "Пример:\n",
        "Рассмотрим коллекцию из двух документов:\n",
        "- Документ 1: \"Кошка сидит на окне.\"\n",
        "- Документ 2: \"Собака бегает по улице.\"\n",
        "\n",
        "Вычислим TF-IDF для слова \"кошка\" в Документе 1:\n",
        "$$ \\text{TF}(кошка, \\text{Документ 1}) = \\frac{1}{4} = 0.25 $$\n",
        "$$ \\text{IDF}(кошка, \\{ \\text{Документ 1}, \\text{Документ 2} \\}) = \\log \\left( \\frac{2}{1} \\right) = \\log(2) \\approx 0.301 $$\n",
        "$$ \\text{TF-IDF}(кошка, \\text{Документ 1}) = 0.25 \\cdot 0.301 = 0.07525 $$\n",
        "\n",
        "Таким образом, векторные представления слов и их вариации, такие как мультимножества слов, мультимножества n-грамм и векторы TF-IDF, играют важную роль в NLP задачах, таких как классификация текстов, кластеризация документов, анализ тональности и другие. Понимание этих методов помогает эффективно работать с текстовыми данными и извлекать семантическую информацию из текста."
      ],
      "metadata": {
        "id": "1x7uZqYdJ8Jm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте рассмотрим примеры на Python для каждого из методов: мультимножества слов, мультимножества n-грамм и векторов TF-IDF."
      ],
      "metadata": {
        "id": "CXVRH5hZGcxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мультимножества слов (Bag of Words)\n",
        "\n",
        "Для создания мультимножества слов можно использовать библиотеку sklearn:"
      ],
      "metadata": {
        "id": "yKJ5Xx5fNn8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Пример текстов\n",
        "corpus = [\n",
        "    \"Кошка сидит на окне\",\n",
        "    \"Собака бегает по улице\",\n",
        "    \"Кошка мурлыкает\"\n",
        "]\n",
        "\n",
        "# Создание объекта CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Преобразование текстов в мультимножества слов\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Вывод словаря слов и их индексов\n",
        "print(\"Словарь слов:\")\n",
        "print(vectorizer.vocabulary_)\n",
        "\n",
        "# Вывод мультимножеств слов в виде разреженной матрицы\n",
        "print(\"Мультимножества слов (разреженная матрица):\")\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WqnzkrkNrQf",
        "outputId": "3da7b981-583c-4ddb-9946-906151ab23c0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Словарь слов:\n",
            "{'кошка': 1, 'сидит': 6, 'на': 3, 'окне': 4, 'собака': 7, 'бегает': 0, 'по': 5, 'улице': 8, 'мурлыкает': 2}\n",
            "Мультимножества слов (разреженная матрица):\n",
            "[[0 1 0 1 1 0 1 0 0]\n",
            " [1 0 0 0 0 1 0 1 1]\n",
            " [0 1 1 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мультимножества n-грамм\n",
        "\n",
        "Для работы с мультимножествами n-грамм также используем CountVectorizer, указывая параметр ngram_range:"
      ],
      "metadata": {
        "id": "UXQrxUqWNxbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Пример текстов\n",
        "corpus = [\n",
        "    \"Кошка сидит на окне\",\n",
        "    \"Собака бегает по улице\",\n",
        "    \"Кошка мурлыкает\"\n",
        "]\n",
        "\n",
        "# Создание объекта CountVectorizer для 2-грамм\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "\n",
        "# Преобразование текстов в мультимножества 2-грамм\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Вывод словаря 2-грамм и их индексов\n",
        "print(\"Словарь 2-грамм:\")\n",
        "print(vectorizer.vocabulary_)\n",
        "\n",
        "# Вывод мультимножеств 2-грамм в виде разреженной матрицы\n",
        "print(\"Мультимножества 2-грамм (разреженная матрица):\")\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_w3HaWbN1Pe",
        "outputId": "0f9eb4d2-a56d-4cac-d6d8-d5dbc94ab4c9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Словарь 2-грамм:\n",
            "{'кошка сидит': 2, 'сидит на': 5, 'на окне': 3, 'собака бегает': 6, 'бегает по': 0, 'по улице': 4, 'кошка мурлыкает': 1}\n",
            "Мультимножества 2-грамм (разреженная матрица):\n",
            "[[0 0 1 1 0 1 0]\n",
            " [1 0 0 0 1 0 1]\n",
            " [0 1 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Векторы TF-IDF\n",
        "\n",
        "Для создания векторов TF-IDF можно использовать TfidfVectorizer из sklearn:"
      ],
      "metadata": {
        "id": "8T_JWtNXN5Dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Пример текстов\n",
        "corpus = [\n",
        "    \"Кошка сидит на окне\",\n",
        "    \"Собака бегает по улице\",\n",
        "    \"Кошка мурлыкает\"\n",
        "]\n",
        "\n",
        "# Создание объекта TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Преобразование текстов в векторы TF-IDF\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Вывод словаря слов и их индексов\n",
        "print(\"Словарь слов:\")\n",
        "print(vectorizer.vocabulary_)\n",
        "\n",
        "# Вывод векторов TF-IDF в виде разреженной матрицы\n",
        "print(\"TF-IDF векторы (разреженная матрица):\")\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okRpWHFaN9YF",
        "outputId": "df4eacf3-fd80-4188-a01a-281ce2f1239a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Словарь слов:\n",
            "{'кошка': 1, 'сидит': 6, 'на': 3, 'окне': 4, 'собака': 7, 'бегает': 0, 'по': 5, 'улице': 8, 'мурлыкает': 2}\n",
            "TF-IDF векторы (разреженная матрица):\n",
            "[[0.         0.40204024 0.         0.52863461 0.52863461 0.\n",
            "  0.52863461 0.         0.        ]\n",
            " [0.5        0.         0.         0.         0.         0.5\n",
            "  0.         0.5        0.5       ]\n",
            " [0.         0.60534851 0.79596054 0.         0.         0.\n",
            "  0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Эти примеры демонстрируют основные шаги для создания мультимножеств слов, мультимножеств n-грамм и векторов TF-IDF на Python с использованием библиотеки sklearn. Каждый из этих методов имеет свои особенности и может быть доработан в зависимости от конкретных требований задачи в обработке естественного языка."
      ],
      "metadata": {
        "id": "W3N24muOOFtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### 4. Поиск релевантных документов из корпуса на основе обратных частотностей документов\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6z6t7PLGGc0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Введение\n",
        "\n",
        "Поиск релевантных документов в корпусе текстов является ключевой задачей в области информационного поиска. Одним из важных инструментов, используемых для оценки важности слов в контексте всего корпуса, является обратная частотность документов (Inverse Document Frequency, IDF). IDF позволяет выявить слова, которые наиболее эффективно различают документы друг от друга, что делает их более информативными для поиска и ранжирования документов по их релевантности.\n",
        "\n",
        "#### Что такое обратная частотность документов (IDF)?\n",
        "\n",
        "Обратная частотность документов (IDF) - это мера, используемая для оценки важности слова в контексте корпуса документов. Она вычисляется как логарифм отношения общего числа документов к числу документов, содержащих данное слово. Слова с высоким значением IDF рассматриваются как более редкие и, следовательно, более информативные для поиска и ранжирования.\n",
        "\n",
        "Формула для вычисления IDF слова \\( t \\) в коллекции документов $ D $:\n",
        "$$ \\text{IDF}(t, D) = \\log \\left( \\frac{|D|}{|\\{d \\in D : t \\in d\\}|} \\right) $$\n",
        "\n",
        "Где:\n",
        "- $ |D| $ - общее количество документов в коллекции.\n",
        "- $ |\\{d \\in D : t \\in d\\}| $ - количество документов, содержащих слово $ t $.\n",
        "\n",
        "#### Пример вычисления IDF\n",
        "\n",
        "Рассмотрим пример коллекции из пяти документов:\n",
        "1. \"Кошка сидит на окне\"\n",
        "2. \"Собака бегает по улице\"\n",
        "3. \"Кошка мурлыкает\"\n",
        "4. \"Собака играет с мячом\"\n",
        "5. \"Птица летает в небе\"\n",
        "\n",
        "Вычислим IDF для слова \"кошка\":\n",
        "$$ |D| = 5 $$\n",
        "$$ |\\{d \\in D : \\text{\"кошка\"} \\in d\\}| = 2 $$\n",
        "\n",
        "$$ \\text{IDF}(\\text{\"кошка\"}, D) = \\log \\left( \\frac{5}{2} \\right) \\approx \\log(2.5) \\approx 0.398 $$\n",
        "\n",
        "#### Применение IDF в поиске релевантных документов\n",
        "\n",
        "IDF используется вместе с частотой термина в документе (Term Frequency, TF) для вычисления весов TF-IDF, которые затем используются для ранжирования документов в поисковой системе. Вес TF-IDF для слова \\( t \\) в документе \\( d \\) вычисляется как произведение TF и IDF:\n",
        "\n",
        "\\[ \\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\cdot \\text{IDF}(t, D) \\]\n",
        "\n",
        "Где:\n",
        "- \\( \\text{TF}(t, d) \\) - частота слова \\( t \\) в документе \\( d \\).\n",
        "- \\( \\text{IDF}(t, D) \\) - обратная частотность слова \\( t \\) в коллекции документов \\( D \\).\n",
        "\n",
        "#### Пример TF-IDF\n",
        "\n",
        "Рассмотрим предыдущий пример с коллекцией документов и вычислим TF-IDF для слова \"кошка\" в каждом документе (предполагая, что TF(\"кошка\", документ) равно 1 для всех документов):\n",
        "\n",
        "$$ \\text{TF-IDF}(\\text{\"кошка\"}, \\text{Документ 1}) = 1 \\cdot 0.398 = 0.398 $$\n",
        "$$ \\text{TF-IDF}(\\text{\"кошка\"}, \\text{Документ 2}) = 0 \\cdot 0.398 = 0 $$\n",
        "$$ \\text{TF-IDF}(\\text{\"кошка\"}, \\text{Документ 3}) = 1 \\cdot 0.398 = 0.398 $$\n",
        "$$ \\text{TF-IDF}(\\text{\"кошка\"}, \\text{Документ 4}) = 0 \\cdot 0.398 = 0 $$\n",
        "$$ \\text{TF-IDF}(\\text{\"кошка\"}, \\text{Документ 5}) = 0 \\cdot 0.398 = 0 $$\n",
        "\n",
        "Из этого примера видно, что документы 1 и 3, содержащие слово \"кошка\", имеют более высокий вес TF-IDF, что указывает на их большую релевантность по отношению к запросу, чем другие документы.\n",
        "\n",
        "Таким образом, обратная частотность документов (IDF) играет важную роль в информационном поиске, помогая определить важность слов в контексте корпуса документов. Она используется для вычисления весов TF-IDF, которые в свою очередь используются для ранжирования документов по их релевантности. Понимание и применение IDF помогает улучшить эффективность поисковых систем и систем информационного поиска в целом."
      ],
      "metadata": {
        "id": "vqJpBMaPGeoj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте реализуем пример вычисления IDF и TF-IDF на Python с использованием библиотеки scikit-learn"
      ],
      "metadata": {
        "id": "CNMHNf2xPasc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Пример коллекции документов\n",
        "corpus = [\n",
        "    \"Кошка сидит на окне\",\n",
        "    \"Собака бегает по улице\",\n",
        "    \"Кошка мурлыкает\",\n",
        "    \"Собака играет с мячом\",\n",
        "    \"Птица летает в небе\"\n",
        "]\n",
        "\n",
        "# Создание объекта TfidfVectorizer с настройками по умолчанию\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Преобразование коллекции текстов в TF-IDF матрицу\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Вывод словаря слов и их индексов (это словарь сопоставления слов и индексов в матрице X)\n",
        "print(\"Словарь слов:\")\n",
        "print(vectorizer.vocabulary_)\n",
        "\n",
        "# Вывод TF-IDF матрицы (в плотном формате для удобства просмотра)\n",
        "print(\"TF-IDF матрица:\")\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVWSVMDtPezV",
        "outputId": "6361d509-5970-44ce-9d09-eeaaf3330276"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Словарь слов:\n",
            "{'кошка': 2, 'сидит': 11, 'на': 6, 'окне': 8, 'собака': 12, 'бегает': 0, 'по': 9, 'улице': 13, 'мурлыкает': 4, 'играет': 1, 'мячом': 5, 'птица': 10, 'летает': 3, 'небе': 7}\n",
            "TF-IDF матрица:\n",
            "[[0.         0.         0.42224214 0.         0.         0.\n",
            "  0.52335825 0.         0.52335825 0.         0.         0.52335825\n",
            "  0.         0.        ]\n",
            " [0.52335825 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.52335825 0.         0.\n",
            "  0.42224214 0.52335825]\n",
            " [0.         0.         0.62791376 0.         0.77828292 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.         0.61418897 0.         0.         0.         0.61418897\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.49552379 0.        ]\n",
            " [0.         0.         0.         0.57735027 0.         0.\n",
            "  0.         0.57735027 0.         0.         0.57735027 0.\n",
            "  0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Оценка сходства пар документов с помощью коэффициентов Отиаи и метрики Okapi BM25\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EkR0kFX2GeyU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Оценка сходства документов является важной задачей в информационном поиске и анализе текстов. Для решения этой задачи применяются различные метрики и коэффициенты, которые позволяют оценивать степень семантической или тематической близости между текстами. В этой лекции мы рассмотрим два таких инструмента: коэффициенты Отиаи (Jaccard Index) и метрику Okapi BM25.\n",
        "\n",
        "#### Коэффициенты Отиаи\n",
        "\n",
        "Коэффициенты Отиаи (или Jaccard Index) измеряют сходство между двумя конечными множествами. В контексте анализа текстов коэффициент Отиаи используется для оценки сходства между множествами токенов (слов или других единиц текста) в двух документах.\n",
        "\n",
        "##### Определение коэффициента Отиаи\n",
        "\n",
        "Коэффициент Отиаи для двух множеств $ A $ и $ B $ определяется как отношение размера их пересечения к размеру их объединения:\n",
        "\n",
        "$$ J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|} $$\n",
        "\n",
        "Где:\n",
        "- $ |A \\cap B| $ - количество элементов, присутствующих в обоих множествах $ A $ и $ B $.\n",
        "- $ |A \\cup B| $ - количество уникальных элементов в множествах $ A $ и $ B $ (объединение множеств).\n",
        "\n",
        "##### Пример вычисления коэффициента Отиаи\n",
        "\n",
        "Предположим, у нас есть два предложения:\n",
        "1. \"Кошка сидит на окне.\"\n",
        "2. \"Собака бегает по улице.\"\n",
        "\n",
        "Множества слов для этих предложений:\n",
        "$$ A = \\{\\text{\"Кошка\", \"сидит\", \"на\", \"окне\"}\\} $$\n",
        "$$ B = \\{\\text{\"Собака\", \"бегает\", \"по\", \"улице\"}\\} $$\n",
        "\n",
        "Вычислим коэффициент Отиаи:\n",
        "$$ J(A, B) = \\frac{|\\{\\text{\"сидит\", \"на\", \"окне\"}\\}|}{|\\{\\text{\"Кошка\", \"сидит\", \"на\", \"окне\", \"Собака\", \"бегает\", \"по\", \"улице\"}\\}|} = \\frac{3}{8} = 0.375 $$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FnsY8tx8GgPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(set1, set2):\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union if union != 0 else 0  # handle division by zero\n",
        "\n",
        "# Пример использования:\n",
        "set1 = set(['apple', 'banana', 'orange'])\n",
        "set2 = set(['banana', 'orange', 'kiwi'])\n",
        "similarity = jaccard_similarity(set1, set2)\n",
        "print(f\"Jaccard similarity between set1 and set2: {similarity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbrdLApTRFPI",
        "outputId": "c91444d6-089c-40c0-90ad-02ea4ae048bf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard similarity between set1 and set2: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Часть 2: Метрика Okapi BM25\n",
        "\n",
        "Метрика Okapi BM25 (Best Matching 25) является популярным алгоритмом ранжирования документов в поисковых системах. Она основана на вероятностной модели информации и учитывает как частоту встречаемости слова в документе (TF), так и его обратную частотность в коллекции документов (IDF).\n",
        "\n",
        "##### Определение метрики Okapi BM25\n",
        "\n",
        "Формула для вычисления BM25 между запросом $ q $ и документом $ D $:\n",
        "\n",
        "$$ \\text{BM25}(D, q) = \\sum_{i} \\text{IDF}(q_i) \\cdot \\frac{f(q_i, D) \\cdot (k_1 + 1)}{f(q_i, D) + k_1 \\cdot (1 - b + b \\cdot \\frac{|D|}{\\text{avgdl}})} $$\n",
        "\n",
        "Где:\n",
        "- $ q_i $ - каждое слово из запроса $ q $.\n",
        "- $ f(q_i, D) $ - частота слова $ q_i $ в документе $ D $ (TF).\n",
        "- $ \\text{IDF}(q_i) $ - обратная частотность слова $ q_i $ в коллекции документов.\n",
        "- $ |D| $ - длина документа $ D $ (количество слов в нём).\n",
        "- $ \\text{avgdl} $ - средняя длина документа в коллекции.\n",
        "- $ k_1 $ и $ b $ - настраиваемые параметры, обычно $ k_1 \\in [1.2, 2.0] $ и $ b \\in [0.75, 0.95] $.\n",
        "\n",
        "##### Применение метрики Okapi BM25\n",
        "\n",
        "Метрика Okapi BM25 используется для ранжирования документов по их релевантности к запросу. Высокое значение BM25 указывает на большую вероятность того, что документ релевантен запросу.\n"
      ],
      "metadata": {
        "id": "Od-IEngfQ2vq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте рассмотрим конкретный пример определения метрики Okapi BM25 для слова \"apple\" в документе.\n",
        "\n",
        "Предположим, у нас есть следующие данные:\n",
        "\n",
        "- Запрос $ q $: \"apple\"\n",
        "- Документ $ D $: \"This is an example document containing the word apple. Apple is a fruit.\"\n",
        "\n",
        "Теперь давайте посчитаем BM25 для слова \"apple\" в этом документе.\n",
        "\n",
        "#### Шаг 1: Вычисление $ f(q_i, D) $ — частоты слова $ q_i $ в документе $ D $\n",
        "\n",
        "Слово \"apple\" встречается дважды в документе $ D $.\n",
        "\n",
        "$$ f(\\text{\"apple\"}, D) = 2 $$\n",
        "\n",
        "#### Шаг 2: Вычисление $ \\text{IDF}(q_i) $ — обратной документной частоты слова $ q_i $\n",
        "\n",
        "Предположим, что мы уже рассчитали обратную документную частоту (IDF) для слова \"apple\" в нашей коллекции документов и она составляет, например, $ \\text{IDF}(\\text{\"apple\"}) = 2.5 $.\n",
        "\n",
        "#### Шаг 3: Вычисление средней длины документа $ \\text{avgdl} $\n",
        "\n",
        "Пусть средняя длина документа в нашей коллекции равна $ \\text{avgdl} = 15 $ слов.\n",
        "\n",
        "#### Шаг 4: Вычисление BM25\n",
        "\n",
        "Теперь мы можем использовать формулу BM25 для слова \"apple\" в документе $ D $:\n",
        "\n",
        "$$ \\text{BM25}(\\text{\"apple\"}, D) = \\text{IDF}(\\text{\"apple\"}) \\cdot \\frac{f(\\text{\"apple\"}, D) \\cdot (k_1 + 1)}{f(\\text{\"apple\"}, D) + k_1 \\cdot (1 - b + b \\cdot \\frac{|D|}{\\text{avgdl}})} $$\n",
        "\n",
        "Подставим значения:\n",
        "\n",
        "$$ \\text{BM25}(\\text{\"apple\"}, D) = 2.5 \\cdot \\frac{2 \\cdot (1.5 + 1)}{2 + 1.5 \\cdot (1 - 0.75 + 0.75 \\cdot \\frac{11}{15})} $$\n",
        "\n",
        "Выполним вычисления:\n",
        "\n",
        "$$ \\text{BM25}(\\text{\"apple\"}, D) = 2.5 \\cdot \\frac{2 \\cdot 2.5}{2 + 1.5 \\cdot (0.25 + 0.75 \\cdot 0.733)} $$\n",
        "$$ \\text{BM25}(\\text{\"apple\"}, D) = 2.5 \\cdot \\frac{5}{2 + 1.5 \\cdot (0.25 + 0.54975)} $$\n",
        "$$ \\text{BM25}(\\text{\"apple\"}, D) = 2.5 \\cdot \\frac{5}{2 + 1.5 \\cdot 0.79975} $$\n",
        "$$ \\text{BM25}(\\text{\"apple\"}, D) = 2.5 \\cdot \\frac{5}{2 + 1.199625} $$\n",
        "$$ \\text{BM25}(\\text{\"apple\"}, D) = 2.5 \\cdot \\frac{5}{3.199625} $$\n",
        "$$ \\text{BM25}(\\text{\"apple\"}, D) = 2.5 \\cdot 1.5625 $$\n",
        "$$ \\text{BM25}(\\text{\"apple\"}, D) \\approx 3.90625 $$\n",
        "\n",
        "Таким образом, BM25 для слова \"apple\" в данном документе составляет примерно 3.91.\n",
        "\n",
        "Этот пример демонстрирует, как можно вычислить BM25 для конкретного слова в документе, используя формулу и предварительно рассчитанные значения IDF, средней длины документа и параметры $ k_1 $ и $ b $.\n",
        "\n"
      ],
      "metadata": {
        "id": "8L30WU5gRO6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def calculate_bm25(word, document, collection_term_freqs, avg_doc_length, k1=1.5, b=0.75):\n",
        "    # Check if the word is in the collection term frequencies\n",
        "    if word not in collection_term_freqs or collection_term_freqs[word] == 0:\n",
        "        raise ValueError(f\"Word '{word}' not found in collection term frequencies or has zero frequency.\")\n",
        "\n",
        "    # IDF calculation\n",
        "    idf = math.log((len(collection_term_freqs) - collection_term_freqs[word] + 0.5) / (collection_term_freqs[word] + 0.5))\n",
        "\n",
        "    # TF calculation\n",
        "    tf = document.count(word)\n",
        "\n",
        "    # Document length\n",
        "    doc_length = len(document)\n",
        "\n",
        "    # BM25 calculation\n",
        "    score = idf * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_length / avg_doc_length))))\n",
        "\n",
        "    return score\n",
        "\n",
        "# Пример использования:\n",
        "query_word = \"apple\"\n",
        "document_text = \"This is an example document containing the word apple. Apple is a fruit.\"\n",
        "document_terms = document_text.lower().split()  # Преобразуем текст документа в список токенов\n",
        "collection_term_freqs = {'apple': 10, 'fruit': 5, 'example': 8}  # Пример частот в коллекции\n",
        "avg_doc_length = 15  # Пример средней длины документа в коллекции\n",
        "\n",
        "try:\n",
        "    bm25_score = calculate_bm25(query_word, document_terms, collection_term_freqs, avg_doc_length)\n",
        "    print(f\"BM25 score for '{query_word}' in the document: {bm25_score}\")\n",
        "except ValueError as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGzN_4P9TXHJ",
        "outputId": "16a55b93-120f-4a44-9704-b9c89519736c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: math domain error\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvSsc0AKUsdl",
        "outputId": "855cbdb5-1393-403e-c115-23b251b9a5d6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.25.2)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Пример коллекции документов\n",
        "corpus = [\n",
        "    \"This is the first document\",\n",
        "    \"This document is the second document\",\n",
        "    \"And this is the third one\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "# Преобразуем текст в токены (слова)\n",
        "tokenized_corpus = [doc.lower().split() for doc in corpus]\n",
        "\n",
        "# Создаем инстанс BM25Okapi\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "# Пример запроса\n",
        "query = \"document\"\n",
        "\n",
        "# Преобразуем запрос в токены (слова)\n",
        "tokenized_query = query.lower().split()\n",
        "\n",
        "# Вычисляем релевантность запроса к документам\n",
        "results = bm25.get_scores(tokenized_query)\n",
        "\n",
        "# Выводим результаты\n",
        "for i, score in enumerate(results):\n",
        "    print(f\"BM25 score for document {i+1}: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgT5J7vWUt-_",
        "outputId": "5a16e385-35ff-4b3b-b671-0fd6ba4a4b48"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BM25 score for document 1: 0.0\n",
            "BM25 score for document 2: 0.0\n",
            "BM25 score for document 3: 0.0\n",
            "BM25 score for document 4: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Параметры $ k_1 $ и $ b $ в метрике Okapi BM25 являются настраиваемыми и влияют на её поведение и результаты. Вот как обычно выбираются эти параметры:\n",
        "\n",
        "### Параметр $ k_1 $\n",
        "\n",
        "Параметр $ k_1 $ контролирует насыщение (saturation) оценки TF (частоты термина) в формуле BM25. Большие значения $ k_1 $ увеличивают вес более часто встречающихся слов в документе. Обычно $ k_1 $ выбирается в диапазоне от 1.2 до 2.0. Вот как обычно происходит выбор:\n",
        "\n",
        "- **Малые значения $ k_1 $** (например, около 1.2): уменьшают влияние высоких значений TF, что может быть полезно для документов с большим количеством повторений слова.\n",
        "  \n",
        "- **Большие значения $ k_1 $** (например, около 2.0): усиливают влияние высоких значений TF, что может быть полезно для документов с низким количеством повторений слова.\n",
        "\n",
        "Выбор конкретного значения $ k_1 $ зависит от особенностей конкретной коллекции документов и требований к поисковой системе.\n",
        "\n",
        "### Параметр $ b $\n",
        "\n",
        "Параметр $ b $ контролирует влияние средней длины документа на оценку в BM25. Он регулирует, насколько сильно длина документа должна влиять на оценку. Обычно $ b $ выбирается в диапазоне от 0.75 до 0.95. Вот как обычно происходит выбор:\n",
        "\n",
        "- **Малые значения $ b $** (например, около 0.75): уменьшают влияние длины документа, что может быть полезно для коллекций с разной длиной документов.\n",
        "  \n",
        "- **Большие значения $ b $** (например, около 0.95): усиливают влияние длины документа, что может быть полезно для коллекций с почти одинаковой длиной документов.\n",
        "\n",
        "Выбор конкретного значения $ b $ также зависит от специфики коллекции документов и требований к системе ранжирования.\n",
        "\n",
        "### Как выбирать $ k_1 $ и $ b $\n",
        "\n",
        "Выбор оптимальных значений $ k_1 $ и $ b $ обычно происходит на основе экспериментов и анализа результатов на тестовых данных или в реальной среде:\n",
        "\n",
        "1. **Эмпирические исследования**: Исследование влияния различных значений $ k_1 $ и $ b $ на качество ранжирования в конкретной поисковой системе или задаче.\n",
        "   \n",
        "2. **Кросс-валидация**: Использование кросс-валидации для оценки, как значения $ k_1 $ и $ b $ влияют на производительность модели.\n",
        "\n",
        "3. **Адаптация к конкретным данным**: Некоторые значения $ k_1 $ и $ b $ могут быть более или менее эффективными в зависимости от особенностей коллекции документов (например, размера, типа документов и т.д.).\n",
        "\n",
        "Таким образом, выбор оптимальных значений $ k_1 $ и $ b $ в метрике Okapi BM25 является важной задачей для достижения высокого качества ранжирования в поисковых системах. Это требует адаптации к конкретным данным и тщательного анализа влияния параметров на результаты.\n"
      ],
      "metadata": {
        "id": "F8BMBhgwUCDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Скалярное произведение и его применении в NLP\n",
        "\n",
        "#### Часть 1: Скалярное произведение\n",
        "\n",
        "**Определение скалярного произведения:**\n",
        "\n",
        "Скалярное произведение двух векторов $ \\mathbf{a} $ и $ \\mathbf{b} $ в $ n $-мерном пространстве определяется как сумма произведений их соответствующих компонент:\n",
        "\n",
        "$$ \\langle \\mathbf{a}, \\mathbf{b} \\rangle = \\sum_{i=1}^{n} a_i b_i $$\n",
        "\n",
        "где $ a_i $ и $ b_i $ — это $ i $-ые компоненты векторов $ \\mathbf{a} $ и $ \\mathbf{b} $ соответственно.\n",
        "\n",
        "**Применение в NLP:**\n",
        "\n",
        "В области обработки естественного языка (NLP) скалярное произведение играет важную роль, особенно при работе с векторными представлениями слов и текстов.\n",
        "\n",
        "#### Часть 2: Измерение пересечений мультимножеств слов в NLP\n",
        "\n",
        "**Определение мультимножества:**\n",
        "\n",
        "Мультимножество (или мультикомплект) представляет собой расширение понятия множества, позволяющее содержать несколько экземпляров одного и того же элемента.\n",
        "\n",
        "**Измерение пересечений мультимножеств слов:**\n",
        "\n",
        "В NLP часто возникает задача измерения сходства между текстами на основе пересечений мультимножеств слов, которые в них содержатся. Рассмотрим примеры и формулы для этого измерения.\n",
        "\n",
        "**Пример 1: Пересечение мультимножеств**\n",
        "\n",
        "Пусть у нас есть два текста, представленных как мультимножества слов $ A $ и $ B $. Для измерения их схожести используется коэффициент Жаккара:\n",
        "\n",
        "$$ J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|} $$\n",
        "\n",
        "где $ |A \\cap B| $ — количество общих слов, а $ |A \\cup B| $ — количество уникальных слов в обоих текстах.\n",
        "\n",
        "**Пример 2: Использование скалярного произведения**\n",
        "\n",
        "Другой подход к измерению схожести текстов основан на векторных представлениях мультимножеств слов. Пусть $ \\mathbf{v}_A $ и $ \\mathbf{v}_B $ — векторные представления мультимножеств слов $ A $ и $ B $. Тогда косинусное расстояние между этими векторами можно выразить через скалярное произведение:\n",
        "\n",
        "$$ \\text{sim}(A, B) = \\frac{\\langle \\mathbf{v}_A, \\mathbf{v}_B \\rangle}{\\|\\mathbf{v}_A\\| \\|\\mathbf{v}_B\\|} $$\n",
        "\n",
        "где $ \\langle \\mathbf{v}_A, \\mathbf{v}_B \\rangle $ — скалярное произведение векторов $ \\mathbf{v}_A $ и $ \\mathbf{v}_B $, $ \\|\\mathbf{v}_A\\| $ и $ \\|\\mathbf{v}_B\\| $ — их нормы.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2Jll8SyRYeLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Хорошо, давай рассмотрим конкретные примеры применения скалярного произведения и измерения пересечений мультимножеств слов в NLP.\n",
        "\n",
        "### Примеры\n",
        "\n",
        "#### Пример 1: Измерение схожести текстов с использованием косинусного расстояния\n",
        "\n",
        "Предположим, у нас есть два предложения:\n",
        "\n",
        "- Текст 1: \"Кошка на ковре\"\n",
        "- Текст 2: \"Собака на ковре\"\n",
        "\n",
        "**Шаги для решения:**\n",
        "\n",
        "1. **Токенизация и создание мультимножеств слов:**\n",
        "\n",
        "   Разобъем каждое предложение на отдельные слова и создадим мультимножества (которые могут содержать повторяющиеся слова):\n",
        "\n",
        "   - Для текста 1: $ A = \\{\\text{\"Кошка\"}, \\text{\"на\"}, \\text{\"ковре\"}\\} $\n",
        "   - Для текста 2: $ B = \\{\\text{\"Собака\"}, \\text{\"на\"}, \\text{\"ковре\"}\\} $\n",
        "\n",
        "2. **Представление в виде векторов:**\n",
        "\n",
        "   Каждое мультимножество представим вектором, где каждая компонента отражает частоту встречаемости слова в тексте:\n",
        "\n",
        "   - Вектор $ \\mathbf{v}_A = [1, 1, 1] $ для текста 1 (порядок: \"Кошка\", \"на\", \"ковре\")\n",
        "   - Вектор $ \\mathbf{v}_B = [1, 1, 1] $ для текста 2 (порядок: \"Собака\", \"на\", \"ковре\")\n",
        "\n",
        "3. **Вычисление скалярного произведения:**\n",
        "\n",
        "   Теперь вычислим скалярное произведение векторов $ \\mathbf{v}_A $ и $ \\mathbf{v}_B $:\n",
        "\n",
        "   $$ \\langle \\mathbf{v}_A, \\mathbf{v}_B \\rangle = 1 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1 = 3 $$\n",
        "\n",
        "4. **Нормализация и вычисление косинусного расстояния:**\n",
        "\n",
        "   Вычислим нормы векторов $ \\mathbf{v}_A $ и $ \\mathbf{v}_B $:\n",
        "\n",
        "   $$ \\|\\mathbf{v}_A\\| = \\sqrt{1^2 + 1^2 + 1^2} = \\sqrt{3} $$\n",
        "   $$ \\|\\mathbf{v}_B\\| = \\sqrt{1^2 + 1^2 + 1^2} = \\sqrt{3} $$\n",
        "\n",
        "   Теперь вычислим косинусное расстояние:\n",
        "\n",
        "   $$ \\text{sim}(A, B) = \\frac{\\langle \\mathbf{v}_A, \\mathbf{v}_B \\rangle}{\\|\\mathbf{v}_A\\| \\|\\mathbf{v}_B\\|} = \\frac{3}{\\sqrt{3} \\cdot \\sqrt{3}} = \\frac{3}{3} = 1 $$\n",
        "\n",
        "   Значение 1 означает полное совпадение векторных представлений текстов, что соответствует их полному пересечению мультимножеств слов.\n",
        "\n",
        "#### Пример 2: Использование коэффициента Жаккара для измерения схожести текстов\n",
        "\n",
        "Пусть у нас есть два предложения:\n",
        "\n",
        "- Текст 1: \"Кошка на ковре\"\n",
        "- Текст 2: \"Кошка на ковре и собака\"\n",
        "\n",
        "**Шаги для решения:**\n",
        "\n",
        "1. **Токенизация и создание мультимножеств слов:**\n",
        "\n",
        "   Разобъем каждое предложение на отдельные слова и создадим мультимножества:\n",
        "\n",
        "   - Для текста 1: $ A = \\{\\text{\"Кошка\"}, \\text{\"на\"}, \\text{\"ковре\"}\\} $\n",
        "   - Для текста 2: $ B = \\{\\text{\"Кошка\"}, \\text{\"на\"}, \\text{\"ковре\"}, \\text{\"и\"}, \\text{\"собака\"}\\} $\n",
        "\n",
        "2. **Вычисление коэффициента Жаккара:**\n",
        "\n",
        "   Вычислим коэффициент Жаккара для текстов 1 и 2:\n",
        "\n",
        "   $$ J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|} = \\frac{3}{5} = 0.6 $$\n",
        "\n",
        "   Это значение (0.6) показывает, что 60% слов из мультимножества текста 1 встречаются также в мультимножестве текста 2.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LawbqzjZYfld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример 1: Вычисление скалярного произведения и косинусного расстояния"
      ],
      "metadata": {
        "id": "3y7j9hWUY1iE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Пример текстов\n",
        "text1 = \"Кошка на ковре\"\n",
        "text2 = \"Собака на ковре\"\n",
        "\n",
        "# Создаем векторизатор для подсчета частот слов\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Преобразуем тексты в векторные представления\n",
        "X = vectorizer.fit_transform([text1, text2])\n",
        "\n",
        "# Получаем матрицу признаков (для наглядности)\n",
        "print(\"Матрица признаков:\")\n",
        "print(X.toarray())\n",
        "\n",
        "# Вычисляем косинусное расстояние между векторами текстов\n",
        "cosine_sim = cosine_similarity(X)\n",
        "print(\"\\nКосинусное расстояние между текстами:\")\n",
        "print(cosine_sim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVeqDKhyY5sv",
        "outputId": "39e6f4e9-5afb-49d7-ce94-5e490c435ea9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Матрица признаков:\n",
            "[[1 1 1 0]\n",
            " [1 0 1 1]]\n",
            "\n",
            "Косинусное расстояние между текстами:\n",
            "[[1.         0.66666667]\n",
            " [0.66666667 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Представленная информация описывает матрицу признаков и вычисленное косинусное расстояние между двумя текстами.\n",
        "\n",
        "Матрица признаков:\n",
        "```\n",
        "[[1 1 1 0]\n",
        " [1 0 1 1]]\n",
        "```\n",
        "Эта матрица содержит двоичные значения, которые могут представлять, например, наличие (1) или отсутствие (0) определенных слов в текстах. Каждая строка соответствует одному тексту, а каждый столбец - одному признаку (слову).\n",
        "\n",
        "Косинусное расстояние:\n",
        "```\n",
        "[[1.         0.66666667]\n",
        " [0.66666667 1.        ]]\n",
        "```\n",
        "Косинусное расстояние - это мера сходства между двумя векторами, которая вычисляется как косинус угла между ними. Значение 1 означает, что два вектора (в данном случае, тексты) идентичны, а значение 0 означает, что они ортогональны (т.е. не имеют общих признаков).\n",
        "\n",
        "Интерпретация полученных значений:\n",
        "1. Косинусное расстояние между первым и вторым текстом равно 0.66666667. Это означает, что эти тексты имеют умеренное сходство, но не идентичны.\n",
        "2. Косинусное расстояние между первым текстом и самим собой равно 1, что ожидаемо, так как текст полностью совпадает сам с собой.\n",
        "3. Косинусное расстояние между вторым текстом и самим собой также равно 1, по той же причине.\n",
        "\n"
      ],
      "metadata": {
        "id": "QDEqqRN4ZdsD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример 2: Вычисление коэффициента Жаккара"
      ],
      "metadata": {
        "id": "EkmZEnwfY8MM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для вычисления коэффициента Жаккара\n",
        "def jaccard_similarity(set1, set2):\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union if union != 0 else 0  # избегаем деления на ноль\n",
        "\n",
        "# Пример текстов\n",
        "text1 = \"Кошка на ковре\"\n",
        "text2 = \"Кошка на ковре и собака\"\n",
        "\n",
        "# Разбиваем тексты на множества слов\n",
        "set1 = set(text1.lower().split())\n",
        "set2 = set(text2.lower().split())\n",
        "\n",
        "# Вычисляем коэффициент Жаккара\n",
        "jaccard = jaccard_similarity(set1, set2)\n",
        "\n",
        "# Выводим результат\n",
        "print(f\"Коэффициент Жаккара между текстами: {jaccard}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD-skxpAZC_m",
        "outputId": "4196e34b-17a4-4840-c374-8d7637cec273"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Коэффициент Жаккара между текстами: 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Значение коэффициента Жаккара между двумя текстами равно 0.6, что означает следующее:\n",
        "\n",
        "1. Коэффициент Жаккара измеряется в диапазоне от 0 до 1, где 0 означает, что два множества (в данном случае, два текста) не имеют общих элементов (слов), а 1 означает, что два множества полностью совпадают.\n",
        "\n",
        "2. Значение 0.6 указывает, что два текста имеют 60% общих элементов (слов) от общего количества уникальных элементов (слов) в обоих текстах.\n",
        "\n",
        "Другими словами, 60% слов, содержащихся в обоих текстах, являются общими, а 40% слов различаются между текстами.\n",
        "\n",
        "Этот показатель может быть полезен для оценки сходства или различия между двумя текстовыми документами, что может быть важно в задачах классификации, кластеризации, поиска дубликатов и т.д."
      ],
      "metadata": {
        "id": "30cIfXdgZRqx"
      }
    }
  ]
}